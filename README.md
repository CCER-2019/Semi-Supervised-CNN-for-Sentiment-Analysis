# Semi-Supervised-CNN-for-Sentiment-Analysis

In This repository，I implemented three Convolutional Neural Network Models for Sentiment Analysis of the Stanford Sentiment Analysis Treebank in tensorflow (python).

1. This first one is 128_hidden_then_softmax.py, which is a model can be seen as BOW-Sentence-Embeding. In other words, each sentence is represented by a vecter of length |V| (the size of the dictionary, say 15000). The indexes of words that in the sentence are set to 1, and all the other places are zeros. Let's see an simple example of this "many-hot" representation. Suppose you dictionary only have 10 words, and words "this", "is", "a" and "test" are indexed at positions 0, 1, 2, 3 respectively. Then the we use a vector of 1111000000 to represent the sentence "this is a test". Then the input layer connects a hidden layer of 128 neurons, and finnaly we use softmax to connect the 128 neurons to the two output neurons (positive and negitive).

Obviously, we lose the word order of the sentence, but it's worth to see how good it can achieve, especially compared to the complexed models we are going to imprement next. We train this model on Stanford Sentiment Analysis TreeBank data, and limit the vocabulary size to 20000, so that only the most frequent words will be considered and all the other words will be replaced by the word "UNK", i.e., zero vector. We use stochastic gradient descent to find the minimum cost of the cross-entropy between the predictions and true lables. In other words, in each iteration, we only use a small batch of the training data, and pretend that is the whole dataset, and use the gradient of this small batch to update our parameters. This model achieves 75% accuaracy on the development data set. After we add another 10,000 training data from the Pang/Lee Rotten Tomatoes dataset, we achieve 86% accuracy, which is no surprise since no model can guaranteed to predict something correctly if it never saw that kind of pattern. 

you can run this model by just type "python 128_hidden_then_softmax.py".

2. The second model I implemented is the idea of the paper "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks". Specifically, I implemented its seq-CNN for sentiment analysis. Each sentence is represented by a |D|*1 vector, where |D| is the number of words in the sentence. Thus each word can be seen as a pixel in the image, and the only difference is that the width of this "sentence-image" is 1. Each pixel has |V| channels (the size of dictionary ), and only one of the channel is 1, and all the other channels are 0s. Then comes to the convolutional layer. we use 32 filters to each region of two consecutive words, learning 32 features of each region. Then comes to the max-pooling layer, and finnaly the softmax layer. Unlike the images, the length of sentence is a variable, but we still want the final full-connection layer has fixed number of neurons. Thus, we pad with 0s each sentence to the length of max sentence length in the dataset, i.e., 64. 

After 20,000 iterations of SGD on the training data, we obtain 84% accuracy on the dev dataset, and 87% on the test dataset.

3. The third model I implemented is the idea of the paper "Semi-supervised convolutional neural networks for text categorization via region embedding", which is an improvement from model 2. Now it also uses unlabled data to learn the region embedding by predicting the context region based on the target region. Thus, when trying to predict the sentiment of a sentence, we have two embeddings of each region. One comes from the supervised part, same as the output of convoluation layer of model 2, and the other one comes from the unsupervised part. 

The region embedding learning from unlabled data is a little bit different from the word embedding learning. For word embedding, the embedding of each word are stored in the parameter W between the input layer and hidden layer. W is a V*H tensor, where V is the dictionary size and H is the number of neurons in the hidden layer. Thus, each row of W is the word for that word. However, for region embedding learning, we use the output the convolution layer as the embedding for each region. If we set the output channel of the convolution layer to 128, then we have a vection of length 128 to represent each region. Thus, we can first train our unsupervised region embedding model, and store the parameters between the input layer and convolution layer (checkpoint), and then when train our supervised model of this paper, we restore model of unsupervised part, and combine with model 2 to satisfy this equation: σ (W · r`(x) + V · u`(x) + b). Now our model will learning the parameters for W, V, b, and of course the parameters of other layers. 

The idea of the learning region embeddings is very useful, since "no context, no meaning". We cannot determine the meaning of of a word without any context information. For example, it is hard to say what does "bank" means if we only see this word. We cannot even determine whether it is a noun and a verb. But we know "bank" means and institute if we see "go to the bank" or "bank of america", and means a mass of a particular substance when we see "a bank of cloud". This is the main reason the region embedding is better than word embedding. Word embedding cannot solve the problem of Polysemy, because each word only have one vector representation regardless of how many meanings that word can have. 

However, 
